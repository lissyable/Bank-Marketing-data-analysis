---
title: "Bank Marketing_Decision Tree"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:


Importing the dataset
```{r}
bank <- read.csv("bank.csv",header = TRUE, sep = ",")
```


```{r}
summary(bank)
```


```{r}
str(bank)
```



```{r}
bank$balance <- as.numeric(bank$balance)
bank$day <- as.numeric(bank$day)
bank$duration <- as.numeric(bank$duration)
bank$campaign <- as.numeric(bank$campaign)
bank$pdays <- as.numeric(bank$pdays)
bank$previous <- as.numeric(bank$previous)
```
 Splitting the dataset into training set and test set
```{r}
# install.packages("caTools")
library(caTools)
set.seed(123)
split = sample.split(bank$y, SplitRatio = 2/3)
training_set = subset(bank, split == TRUE)
test_set = subset(bank, split == FALSE)
```

Fitting logistic regression to the training test
```{r}
# install.packages("rpart")
library(rpart)
classifier = rpart(y ~ ., data = training_set)
classifier
```
Plot the data
```{r}
plot(classifier, margin = 0.1)
text(classifier, use.n = TRUE, pretty = TRUE, cex = 0.8)
```


Predicting the test results
```{r}
y_pred = predict(classifier, newdata = test_set, type = "class")
y_pred
```
Confusion matrix
```{r}
cm = table(test_set$y, y_pred)
cm
```
Accuracy is the ratio of correctly predicted observation to the total observations
```{r}
accuracy <- sum(diag(cm))/sum(cm) 
accuracy
```
 So we got 90% accuracy
Precision is the ratio of correctly predicted positive observations to the total predicted positive observations.
```{r}
precision <- cm[2,2]/(sum(cm[,2]))
precision
```
We got 62% precision
Recall is the ratio of correctly predicted positive observations to the all observations in actual class
```{r}
recall <- cm[2,2]/(sum(cm[2,]))
recall
```
We got 33% recall

```{r}
F1 <- 2*(recall * precision) / (recall + precision)
F1
```

 43% is the F1 score

We are removing age, job, previous, default, pdays, balance
The new dataset becoomes
```{r}
bank <- bank[c(3:4, 7:13, 16:17)]
str(bank)
```

 Splitting the dataset into training set and test set
```{r}
set.seed(123)
split = sample.split(bank$y, SplitRatio = 2/3)
training_set = subset(bank, split == TRUE)
test_set = subset(bank, split == FALSE)
```

Fitting logistic regression to the training test
```{r}
classifier = glm(y ~ ., family = binomial, data = training_set)
summary(classifier)
```

Predicting the test results
```{r}
prob_pred = predict(classifier, type = 'response', newdata = test_set)
y_pred = ifelse(prob_pred > 0.5, 1, 0)
y_pred
```
Confusion matrix

```{r}
cm = table(test_set$y, y_pred)
cm
```
Accuracy is the ratio of correctly predicted observation to the total observations
```{r}
accuracy <- sum(diag(cm))/sum(cm) 
accuracy
```
 So we got 90% accuracy
Precision is the ratio of correctly predicted positive observations to the total predicted positive observations.
```{r}
precision <- cm[2,2]/(sum(cm[,2]))
precision
```
We got 66% precision
Recall is the ratio of correctly predicted positive observations to the all observations in actual class
```{r}
recall <- cm[2,2]/(sum(cm[2,]))
recall
```
We got 34% recall

```{r}
F1 <- 2*(recall * precision) / (recall + precision)
F1
```

 45% is the F1 score





